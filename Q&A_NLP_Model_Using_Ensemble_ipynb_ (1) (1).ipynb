{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "04teCf2lRBdQ",
        "czW3VUi_UqbD",
        "cdSanS23Va2G",
        "pdKZp2x97aLK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Prepare Data"
      ],
      "metadata": {
        "id": "04teCf2lRBdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install evaluate -q\n",
        "!pip install optuna -q\n",
        "#!pip install --upgrade accelerate"
      ],
      "metadata": {
        "id": "q1Ski887Sta3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "import evaluate\n",
        "import optuna\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_HfCyUAERF1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chekpoints = [\"timpal0l/mdeberta-v3-base-squad2\", \"bert-large-uncased-whole-word-masking-finetuned-squad\",\"deepset/roberta-base-squad2\"]\n",
        "\n",
        "models = []\n",
        "tokenizers = []\n",
        "for chekpoint in chekpoints:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(chekpoint)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(chekpoint)\n",
        "    tokenizers.append(tokenizer)\n",
        "    models.append(model)"
      ],
      "metadata": {
        "id": "DoqTbDTL1zZ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"sberquad\")"
      ],
      "metadata": {
        "id": "Nj3pwkesYmRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = dataset[\"train\"][2][\"context\"]\n",
        "question = dataset[\"train\"][2][\"question\"]\n",
        "\n",
        "inputs = tokenizer(question, context)\n",
        "tokenizer.decode(inputs[\"input_ids\"])"
      ],
      "metadata": {
        "id": "cIzQoWsmS9gJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 64 1024\n",
        "\n",
        "stride=128\n",
        "max_seq_length = 384\n",
        "def preprocess_examples(examples):\n",
        "    questions = [q.strip() for q in examples['question']]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples['context'],\n",
        "        max_length=max_seq_length,\n",
        "        truncation='only_second',\n",
        "        stride=stride,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length',\n",
        "        )\n",
        "\n",
        "    offset_mapping = inputs['offset_mapping']\n",
        "    answers = examples['answers']\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer['answer_start'][0]\n",
        "        end_char = answer['answer_start'][0] + len(answer['text'][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "    # Find the start and end of the context\n",
        "\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "     # If the answer is not fully inside the context, label is (0, 0)\n",
        "\n",
        "        if offset[context_start][0] > end_char \\\n",
        "            or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "\n",
        "      # Otherwise it's the start and end token positions\n",
        "\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs['start_positions'] = start_positions\n",
        "    inputs['end_positions'] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "iF-JIx5CTBHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#–î–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –ø—Ä–æ—Ü–µ—Å—Å–∞ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ —É–º–µ–Ω—å—à–∏–º –¥–∞—Ç–∞—Å–µ—Ç\n",
        "part_of_data = 0.05\n",
        "DATASETS_for_optuna = DatasetDict({\n",
        "    'train': dataset[\"train\"].map(\n",
        "        preprocess_examples,\n",
        "       batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"train\"])), int(len(dataset[\"train\"])*part_of_data), replace=False)\n",
        "        ),\n",
        "    'validation': dataset[\"validation\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"validation\"])), int(len(dataset[\"validation\"])*part_of_data), replace=False)\n",
        "        ),\n",
        "    'test': dataset[\"test\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"test\"])), int(len(dataset[\"test\"])*part_of_data), replace=False)\n",
        "        )\n",
        "})\n",
        "DATASETS_for_optuna"
      ],
      "metadata": {
        "id": "j3un5miXTsmT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASETS = DatasetDict({\n",
        "    'train': dataset[\"train\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"train\"])), int(len(dataset[\"train\"])), replace=False)\n",
        "        ),\n",
        "    'validation': dataset[\"validation\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"validation\"])), int(len(dataset[\"validation\"])), replace=False)\n",
        "        ),\n",
        "    'test': dataset[\"test\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"test\"])), int(len(dataset[\"test\"])), replace=False)\n",
        "        )\n",
        "})\n",
        "DATASETS"
      ],
      "metadata": {
        "id": "SyjFspoST0Y3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ú–µ—Ç—Ä–∏–∫–∏"
      ],
      "metadata": {
        "id": "czW3VUi_UqbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"squad\")\n",
        "def compute_metrics_for_optuna(eval_preds):\n",
        "    y_pred = np.argmax(eval_preds[0], -1).T\n",
        "\n",
        "    f1_score = 0\n",
        "    exact_match = 0\n",
        "    for data, pred in zip(DATASETS_for_optuna['validation'], y_pred):\n",
        "        # Convert answer start and end into characters positions in\n",
        "        # original text using the offset mapping list\n",
        "        start_char = data['offset_mapping'][pred[0]][0]\n",
        "        end_char = data['offset_mapping'][pred[1]][1]\n",
        "        # Create predictions and references dictionaries for metric function\n",
        "        predictions = [{'prediction_text': data['context'][start_char:end_char],\n",
        "                        'id': str(data['id'])}]\n",
        "        references = [{'answers': data['answers'],\n",
        "                       'id': str(data['id'])}]\n",
        "        results = metric.compute(predictions=predictions,\n",
        "                                 references=references)\n",
        "        # Add metric to running sum variable to calculate average after,\n",
        "        # change outputs from 0-100 range to 0-1 range\n",
        "        f1_score += results['f1'] / 100\n",
        "        exact_match += results['exact_match'] / 100\n",
        "    # Calculate the average\n",
        "    f1_score /= len(DATASETS_for_optuna['validation'])\n",
        "    exact_match /= len(DATASETS_for_optuna['validation'])\n",
        "    return {'f1': f1_score, 'exact_match': exact_match}"
      ],
      "metadata": {
        "id": "NxXiMJBAUc2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    y_pred = np.argmax(eval_preds[0], -1).T\n",
        "\n",
        "    f1_score = 0\n",
        "    exact_match = 0\n",
        "    for data, pred in zip(DATASETS['validation'], y_pred):\n",
        "        # Convert answer start and end into characters positions in\n",
        "        # original text using the offset mapping list\n",
        "        start_char = data['offset_mapping'][pred[0]][0]\n",
        "        end_char = data['offset_mapping'][pred[1]][1]\n",
        "        # Create predictions and references dictionaries for metric function\n",
        "        predictions = [{'prediction_text': data['context'][start_char:end_char],\n",
        "                         'id': str(data['id'])}]\n",
        "        references = [{'answers': data['answers'],\n",
        "                       'id': str(data['id'])}]\n",
        "        results = metric.compute(predictions=predictions,\n",
        "                                 references=references)\n",
        "        # Add metric to running sum variable to calculate average after,\n",
        "        # change outputs from 0-100 range to 0-1 range\n",
        "        f1_score += results['f1'] / 100\n",
        "        exact_match += results['exact_match'] / 100\n",
        "    # Calculate the average\n",
        "    f1_score /= len(DATASETS['validation'])\n",
        "    exact_match /= len(DATASETS['validation'])\n",
        "    return {'f1': f1_score, 'exact_match': exact_match}"
      ],
      "metadata": {
        "id": "j7YusbuzVH41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "cdSanS23Va2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
        "LR_MIN = 4e-5\n",
        "LR_CEIL = 0.01\n",
        "WD_MIN = 4e-5\n",
        "WD_CEIL = 0.01\n",
        "WR_MIN = 0.01\n",
        "WR_CEIL = 0.2\n",
        "MIN_GRAD_ACC = 1\n",
        "MAX_GRAD_ACC = 5\n",
        "MIN_EPOCHS = 2\n",
        "MAX_EPOCHS = 5\n",
        "PER_DEVICE_EVAL_BATCH = 10\n",
        "PER_DEVICE_TRAIN_BATCH = 10\n",
        "NUM_TRIALS = 3\n",
        "SAVE_DIR = 'opt-test'\n",
        "NAME_OF_MODEL = 'huggingoptunaface'"
      ],
      "metadata": {
        "id": "ZLXEm3KEVYLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers[torch] accelerate"
      ],
      "metadata": {
        "id": "8bkWem1fYBcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U accelerate\n",
        "! pip install -U transformers"
      ],
      "metadata": {
        "id": "hBpfiFnnaKS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import accelerate\n",
        "import transformers\n",
        "\n",
        "transformers.__version__, accelerate.__version__"
      ],
      "metadata": {
        "id": "vzn6kmAtaBif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0002839561415811\n",
        "weight_decay = 6.811462411625139e-05\n",
        "warmup_ratio = 0.1283623708167592\n",
        "gradient_accumulation_step = 1\n",
        "epoch = 3"
      ],
      "metadata": {
        "id": "aG0qvUzef5dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "model_names = ['model1', 'model2', 'model3']\n",
        "\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results', # output directory\n",
        "    num_train_epochs=epoch, # total number of training epochs\n",
        "    per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH//4, # batch size per device during training\n",
        "    per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH//4, # batch size for evaluation\n",
        "    warmup_steps=200, # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=weight_decay, # strength of weight decay\n",
        "    logging_dir='./logs', # directory for storing logs\n",
        "    logging_steps=1000,\n",
        "    save_steps=5000,\n",
        "    fp16=True, # if you want to use mixed precision training, requires NVIDIA Apex library\n",
        ")\n",
        "\n",
        "trainers = []\n",
        "for i, (model, tokenizer) in enumerate(zip(models, tokenizers)):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    torch.cuda.empty_cache()\n",
        "    model.to(device)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model, # the instantiated ü§ó Transformers model to be trained\n",
        "        args=training_args, # training arguments, defined above\n",
        "        train_dataset=DATASETS['train'], # training dataset\n",
        "        eval_dataset=DATASETS['validation'], # evaluation dataset\n",
        "        tokenizer=tokenizer, # this is important to ensure the tokenizer is saved along with the model\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainers.append(trainer)\n",
        "\n",
        "    trainer.save_model(f'{model_names[i]}_chekpoint') # Saves the model into the model_dir defined in our TrainingArguments\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n"
      ],
      "metadata": {
        "id": "nRkWrjDsWA21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Prediction"
      ],
      "metadata": {
        "id": "pdKZp2x97aLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def ensemble_predictions(*predictions):\n",
        "    # Simply takes the mean of the predictions\n",
        "    # You might want to consider weighted mean, voting etc.\n",
        "    return sum(predictions) / len(predictions)\n",
        "\n",
        "# –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏\n",
        "all_predictions = []\n",
        "for trainer in trainers:\n",
        "    predictions, _, _ = trainer.predict(DATASETS['test'])\n",
        "    all_predictions.append(predictions)\n",
        "\n",
        "# –ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤\n",
        "ensemble_preds = ensemble_predictions(*all_predictions)\n",
        "\n",
        "# –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–æ–≤\n",
        "y_pred = np.argmax(ensemble_preds, -1).T\n",
        "\n",
        "f1_score = 0\n",
        "exact_match = 0\n",
        "example_predictions = []\n",
        "example_references = []\n",
        "for data, pred in zip(DATASETS['test'], y_pred):\n",
        "    start_char = data['offset_mapping'][pred[0]][0]\n",
        "    end_char = data['offset_mapping'][pred[1]][1]\n",
        "\n",
        "    predictions = [{'prediction_text': data['context'][start_char:end_char],\n",
        "                     'id': str(data['id'])}]\n",
        "    references = [{'answers': data['answers'],\n",
        "                   'id': str(data['id'])}]\n",
        "    example_predictions.append(predictions[0]['prediction_text'])\n",
        "    example_references.append(references[0]['answers']['text'][0])\n",
        "\n",
        "    results = metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    f1_score += results['f1'] / 100\n",
        "    exact_match += results['exact_match'] / 100\n",
        "\n",
        "# –ü–æ—Å—á–∏—Ç–∞—Ç—å —Å—Ä–µ–¥–Ω—é—é —Ç–æ—á–Ω–æ—Å—Ç—å\n",
        "f1_score /= len(DATASETS['test'])\n",
        "exact_match /= len(DATASETS['test'])\n",
        "\n",
        "print(f\"Ensemble F1 Score: {f1_score}\")\n",
        "print(f\"Ensemble Exact Match: {exact_match}\")\n",
        "\n",
        "# –í—ã–≤–æ–¥ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π\n",
        "print(\"\\nRandom Predicted Examples from Ensemble:\")\n",
        "for pred, ref in zip(example_predictions[:5], example_references[:5]):\n",
        "    print(f\"Ref Answer: {ref}\\nPred Answer: {pred}\\n\")\n"
      ],
      "metadata": {
        "id": "_oDqN7Zeghof"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}