{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "zalhuAtSGmDV",
        "1FtWevUBGuK_"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Prepare Data"
      ],
      "metadata": {
        "id": "zalhuAtSGmDV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AMRhTs3nNcy",
        "outputId": "d0caa3f0-5808-4a8c-a79a-de18876cd29d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.1.0+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install evaluate -q\n",
        "!pip install optuna -q\n",
        "!pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Импорт необходимых пакетов\n",
        "import collections\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import random\n",
        "import time\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# pytorch libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForQuestionAnswering\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "import evaluate\n",
        "import optuna\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "LDD4M8L3nZE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"sberquad\")\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UmnF_rknhEv",
        "outputId": "d1c43e31-b051-4110-9c4c-eccdf87fee70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 45328\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5036\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 23936\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"timpal0l/mdeberta-v3-base-squad2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "zFQ0fByunmZc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context = dataset[\"train\"][2][\"context\"]\n",
        "question = dataset[\"train\"][2][\"question\"]\n",
        "\n",
        "inputs = tokenizer(question, context)\n",
        "tokenizer.decode(inputs[\"input_ids\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "CpdeUbd-no4B",
        "outputId": "3d5644c9-6eb0-40ce-9066-218be12c204f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] что встречается в протерозойских отложениях?[SEP] В протерозойских отложениях органические остатки встречаются намного чаще, чем в архейских. Они представлены известковыми выделениями сине-зелёных водорослей, ходами червей, остатками кишечнополостных. Кроме известковых водорослей, к числу древнейших растительных остатков относятся скопления графито-углистого вещества, образовавшегося в результате разложения Corycium enigmaticum. В кремнистых сланцах железорудной формации Канады найдены нитевидные водоросли, грибные нити и формы, близкие современным кокколитофоридам. В железистых кварцитах Северной Америки и Сибири обнаружены железистые продукты жизнедеятельности бактерий.[SEP]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stride=128\n",
        "max_seq_length = 384\n",
        "\n",
        "def preprocess_examples(examples):\n",
        "    questions = [q.strip() for q in examples['question']]\n",
        "    inputs = tokenizer(\n",
        "        questions,\n",
        "        examples['context'],\n",
        "        max_length=max_seq_length,\n",
        "        truncation='only_second',\n",
        "        stride=stride,\n",
        "        return_offsets_mapping=True,\n",
        "        padding='max_length',\n",
        "        )\n",
        "\n",
        "    offset_mapping = inputs['offset_mapping']\n",
        "    answers = examples['answers']\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "    for i, offset in enumerate(offset_mapping):\n",
        "        answer = answers[i]\n",
        "        start_char = answer['answer_start'][0]\n",
        "        end_char = answer['answer_start'][0] + len(answer['text'][0])\n",
        "        sequence_ids = inputs.sequence_ids(i)\n",
        "\n",
        "    # Find the start and end of the context\n",
        "\n",
        "        idx = 0\n",
        "        while sequence_ids[idx] != 1:\n",
        "            idx += 1\n",
        "        context_start = idx\n",
        "        while sequence_ids[idx] == 1:\n",
        "            idx += 1\n",
        "        context_end = idx - 1\n",
        "\n",
        "    # If the answer is not fully inside the context, label is (0, 0)\n",
        "\n",
        "        if offset[context_start][0] > end_char \\\n",
        "            or offset[context_end][1] < start_char:\n",
        "            start_positions.append(0)\n",
        "            end_positions.append(0)\n",
        "        else:\n",
        "\n",
        "      # Otherwise it's the start and end token positions\n",
        "\n",
        "            idx = context_start\n",
        "            while idx <= context_end and offset[idx][0] <= start_char:\n",
        "                idx += 1\n",
        "            start_positions.append(idx - 1)\n",
        "\n",
        "            idx = context_end\n",
        "            while idx >= context_start and offset[idx][1] >= end_char:\n",
        "                idx -= 1\n",
        "            end_positions.append(idx + 1)\n",
        "\n",
        "    inputs['start_positions'] = start_positions\n",
        "    inputs['end_positions'] = end_positions\n",
        "    return inputs"
      ],
      "metadata": {
        "id": "ifmMnsHUn--y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Для ускорения процесса подбора гиперпараметров уменьшим датасет\n",
        "part_of_data = 0.05\n",
        "DATASETS_for_optuna = DatasetDict({\n",
        "    'train': dataset[\"train\"].map(\n",
        "        preprocess_examples,\n",
        "       batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"train\"])), int(len(dataset[\"train\"])*part_of_data), replace=False)\n",
        "        ),\n",
        "    'validation': dataset[\"validation\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"validation\"])), int(len(dataset[\"validation\"])*part_of_data), replace=False)\n",
        "        ),\n",
        "    'test': dataset[\"test\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"test\"])), int(len(dataset[\"test\"])*part_of_data), replace=False)\n",
        "        )\n",
        "})"
      ],
      "metadata": {
        "id": "PE10ht5uoMkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DATASETS = DatasetDict({\n",
        "    'train': dataset[\"train\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"train\"])), int(len(dataset[\"train\"])), replace=False)\n",
        "        ),\n",
        "    'validation': dataset[\"validation\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"validation\"])), int(len(dataset[\"validation\"])), replace=False)\n",
        "        ),\n",
        "    'test': dataset[\"test\"].map(\n",
        "        preprocess_examples,\n",
        "        batched=True).select(\n",
        "            np.random.choice(range(len(dataset[\"test\"])), int(len(dataset[\"test\"])), replace=False)\n",
        "        )\n",
        "})"
      ],
      "metadata": {
        "id": "xIW4ctDRoNXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Метрики"
      ],
      "metadata": {
        "id": "1FtWevUBGuK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"squad\")\n",
        "def compute_metrics_for_optuna(eval_preds):\n",
        "    y_pred = np.argmax(eval_preds[0], -1).T\n",
        "\n",
        "    f1_score = 0\n",
        "    exact_match = 0\n",
        "    for data, pred in zip(DATASETS_for_optuna['validation'], y_pred):\n",
        "        # Convert answer start and end into characters positions in\n",
        "        # original text using the offset mapping list\n",
        "        start_char = data['offset_mapping'][pred[0]][0]\n",
        "        end_char = data['offset_mapping'][pred[1]][1]\n",
        "        # Create predictions and references dictionaries for metric function\n",
        "        predictions = [{'prediction_text': data['context'][start_char:end_char],\n",
        "                        'id': str(data['id'])}]\n",
        "        references = [{'answers': data['answers'],\n",
        "                       'id': str(data['id'])}]\n",
        "        results = metric.compute(predictions=predictions,\n",
        "                                 references=references)\n",
        "        # Add metric to running sum variable to calculate average after,\n",
        "        # change outputs from 0-100 range to 0-1 range\n",
        "        f1_score += results['f1'] / 100\n",
        "        exact_match += results['exact_match'] / 100\n",
        "    # Calculate the average\n",
        "    f1_score /= len(DATASETS_for_optuna['validation'])\n",
        "    exact_match /= len(DATASETS_for_optuna['validation'])\n",
        "    return {'f1': f1_score, 'exact_match': exact_match}"
      ],
      "metadata": {
        "id": "2O3XRcSRoQ7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "    y_pred = np.argmax(eval_preds[0], -1).T\n",
        "\n",
        "    f1_score = 0\n",
        "    exact_match = 0\n",
        "    for data, pred in zip(DATASETS['validation'], y_pred):\n",
        "        # Convert answer start and end into characters positions in\n",
        "        # original text using the offset mapping list\n",
        "        start_char = data['offset_mapping'][pred[0]][0]\n",
        "        end_char = data['offset_mapping'][pred[1]][1]\n",
        "        # Create predictions and references dictionaries for metric function\n",
        "        predictions = [{'prediction_text': data['context'][start_char:end_char],\n",
        "                        'id': str(data['id'])}]\n",
        "        references = [{'answers': data['answers'],\n",
        "                       'id': str(data['id'])}]\n",
        "        results = metric.compute(predictions=predictions,\n",
        "                                 references=references)\n",
        "        # Add metric to running sum variable to calculate average after,\n",
        "        # change outputs from 0-100 range to 0-1 range\n",
        "        f1_score += results['f1'] / 100\n",
        "        exact_match += results['exact_match'] / 100\n",
        "    # Calculate the average\n",
        "    f1_score /= len(DATASETS['validation'])\n",
        "    exact_match /= len(DATASETS['validation'])\n",
        "    return {'f1': f1_score, 'exact_match': exact_match}"
      ],
      "metadata": {
        "id": "v4CrdsWooeMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "PyxuudCvHHSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Гиперпараметры\n",
        "LR_MIN = 4e-5\n",
        "LR_CEIL = 0.01\n",
        "WD_MIN = 4e-5\n",
        "WD_CEIL = 0.01\n",
        "WR_MIN = 0.01\n",
        "WR_CEIL = 0.2\n",
        "MIN_GRAD_ACC = 1\n",
        "MAX_GRAD_ACC = 5\n",
        "MIN_EPOCHS = 2\n",
        "MAX_EPOCHS = 5\n",
        "PER_DEVICE_EVAL_BATCH = 10\n",
        "PER_DEVICE_TRAIN_BATCH = 10\n",
        "NUM_TRIALS = 3\n",
        "SAVE_DIR = 'opt-test'\n",
        "NAME_OF_MODEL = 'huggingoptunaface'"
      ],
      "metadata": {
        "id": "OCHu6tAxofh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "dc2AqonNol8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.cuda.empty_cache()\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "983TodXSon9C",
        "outputId": "04e22682-7aa3-48d4-a546-3c40bcd1572c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DebertaV2ForQuestionAnswering(\n",
              "  (deberta): DebertaV2Model(\n",
              "    (embeddings): DebertaV2Embeddings(\n",
              "      (word_embeddings): Embedding(251000, 768, padding_idx=0)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "      (dropout): StableDropout()\n",
              "    )\n",
              "    (encoder): DebertaV2Encoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x DebertaV2Layer(\n",
              "          (attention): DebertaV2Attention(\n",
              "            (self): DisentangledSelfAttention(\n",
              "              (query_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (pos_dropout): StableDropout()\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "            (output): DebertaV2SelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "              (dropout): StableDropout()\n",
              "            )\n",
              "          )\n",
              "          (intermediate): DebertaV2Intermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): DebertaV2Output(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "            (dropout): StableDropout()\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (rel_embeddings): Embedding(512, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-07, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.0002839561415811\n",
        "weight_decay = 6.811462411625139e-05\n",
        "warmup_ratio = 0.1283623708167592\n",
        "gradient_accumulation_step = 1\n",
        "epoch = 3"
      ],
      "metadata": {
        "id": "7yVVj_iMoqHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\"mdeberta-v3-base-squad2\",\n",
        "                                  evaluation_strategy=\"steps\",\n",
        "                                  eval_steps=1000,\n",
        "                                  logging_steps=1000,\n",
        "                                  save_steps=5000,\n",
        "                                  optim=\"adamw_torch\",\n",
        "                                  learning_rate=learning_rate,\n",
        "                                  per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH,\n",
        "                                  per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
        "                                  warmup_steps=200,\n",
        "                                  lr_scheduler_type='cosine',\n",
        "                                  weight_decay=weight_decay,\n",
        "                                  warmup_ratio=warmup_ratio,\n",
        "                                  gradient_accumulation_steps=gradient_accumulation_step,\n",
        "                                  num_train_epochs=epoch)\n",
        "\n",
        "trainer = Trainer(model,\n",
        "                  training_args,\n",
        "                  train_dataset=DATASETS['train'],\n",
        "                  eval_dataset=DATASETS['validation'],\n",
        "                  tokenizer=tokenizer,\n",
        "                  compute_metrics=compute_metrics)"
      ],
      "metadata": {
        "id": "XsBBsODOotCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "FSSe1DCuo6mH",
        "outputId": "5c9484fd-cc38-476d-e27b-d9a869533c45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13599' max='13599' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13599/13599 4:30:21, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Exact Match</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.533600</td>\n",
              "      <td>2.395486</td>\n",
              "      <td>0.634880</td>\n",
              "      <td>0.420969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.429700</td>\n",
              "      <td>2.206887</td>\n",
              "      <td>0.674568</td>\n",
              "      <td>0.455123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>2.368100</td>\n",
              "      <td>2.205993</td>\n",
              "      <td>0.671488</td>\n",
              "      <td>0.462669</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>2.317600</td>\n",
              "      <td>2.207917</td>\n",
              "      <td>0.708822</td>\n",
              "      <td>0.487887</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>2.206200</td>\n",
              "      <td>2.095395</td>\n",
              "      <td>0.713921</td>\n",
              "      <td>0.494440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>2.083900</td>\n",
              "      <td>2.037686</td>\n",
              "      <td>0.716817</td>\n",
              "      <td>0.499404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>1.988800</td>\n",
              "      <td>1.937767</td>\n",
              "      <td>0.722898</td>\n",
              "      <td>0.515687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>1.912700</td>\n",
              "      <td>1.903542</td>\n",
              "      <td>0.739591</td>\n",
              "      <td>0.532764</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>1.831000</td>\n",
              "      <td>1.818703</td>\n",
              "      <td>0.752485</td>\n",
              "      <td>0.549047</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>1.604800</td>\n",
              "      <td>1.847346</td>\n",
              "      <td>0.737164</td>\n",
              "      <td>0.533360</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>1.604400</td>\n",
              "      <td>1.756798</td>\n",
              "      <td>0.760559</td>\n",
              "      <td>0.558380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>1.557800</td>\n",
              "      <td>1.739189</td>\n",
              "      <td>0.762540</td>\n",
              "      <td>0.560564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>1.516000</td>\n",
              "      <td>1.754076</td>\n",
              "      <td>0.762314</td>\n",
              "      <td>0.560961</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=13599, training_loss=1.976883267819842, metrics={'train_runtime': 16222.0027, 'train_samples_per_second': 8.383, 'train_steps_per_second': 0.838, 'total_flos': 2.6649614865752064e+16, 'train_loss': 1.976883267819842, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Prediction on Test Data"
      ],
      "metadata": {
        "id": "5aBCMptDHLeg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция тренера для получения прогноза\n",
        "predictions, _, _ = trainer.predict(DATASETS['test'])\n",
        "\n",
        "# Получение прогнозов\n",
        "preds = np.argmax(predictions, axis=2)\n",
        "\n",
        "f1_score = 0\n",
        "exact_match = 0\n",
        "example_predictions = []\n",
        "example_references = []\n",
        "\n",
        "# Прогнозирование на тестовом наборе данных\n",
        "for data, pred in zip(DATASETS['test'], preds):\n",
        "    start_char = data['offset_mapping'][pred[0]][0]\n",
        "    end_char = data['offset_mapping'][pred[1]][1]\n",
        "\n",
        "    predictions = [{'prediction_text': data['context'][start_char:end_char],\n",
        "                     'id': str(data['id'])}]\n",
        "    references = [{'answers': data['answers'],\n",
        "                   'id': str(data['id'])}]\n",
        "    example_predictions.append(predictions[0]['prediction_text'])\n",
        "    example_references.append(references[0]['answers']['text'][0])\n",
        "\n",
        "    results = metric.compute(predictions=predictions, references=references)\n",
        "\n",
        "    f1_score += results['f1'] / 100\n",
        "    exact_match += results['exact_match'] / 100\n",
        "\n",
        "# Расчет средней точности\n",
        "f1_score /= len(DATASETS['test'])\n",
        "exact_match /= len(DATASETS['test'])\n",
        "\n",
        "print(f\"F1 Score: {f1_score}\")\n",
        "print(f\"Exact Match: {exact_match}\")\n",
        "\n",
        "# Вывод некоторых примеров предсказаний\n",
        "print(\"\\nRandom Predicted Examples:\")\n",
        "for pred, ref in zip(example_predictions[:5], example_references[:5]):\n",
        "    print(f\"Ref Answer: {ref}\\nPred Answer: {pred}\\n\")"
      ],
      "metadata": {
        "id": "xnZqnV9RvHcl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "9ca83365-3fcf-4b26-8994-0fb7380a84b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score: 0.0\n",
            "Exact Match: 0.0\n",
            "\n",
            "Random Predicted Examples:\n",
            "Ref Answer: \n",
            "Pred Answer: Наряду с кровеносной системой у позвоночных есть другая, связанная с ней, сосудистая система — лимфатическая. Она состоит из лимфатических сосудов и лимфатических желёз. Ли\n",
            "\n",
            "Ref Answer: \n",
            "Pred Answer:  преобразований в области лексики\n",
            "\n"
          ]
        }
      ]
    }
  ]
}